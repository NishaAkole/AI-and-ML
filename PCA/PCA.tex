\documentclass[journal,12pt,twocolumn]{IEEEtran}

\makeatletter
\makeatother
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{caption}

\singlespacing

%\usepackage{geometry}
%\geometry{letterpaper}

\usepackage{graphicx}
\graphicspath{ {./images}  }
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{wasysym}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{cite}
\usepackage{cases}
\usepackage{mathtools}
\usepackage{subfig}
\usepackage{enumerate}	
\usepackage{enumitem}
\usepackage{amsmath, amssymb}

\usepackage{longtable}
\usepackage{multirow}

\usepackage{enumitem}
\usepackage{mathtools}

\usepackage{listings}
\usepackage{listings}
\usepackage[latin1]{inputenc}                                
\usepackage{color}                                            
\usepackage{array}                                            
\usepackage{longtable}                                        
\usepackage{calc}                                             
\usepackage{multirow}                                        
\usepackage{hhline}                                          
\usepackage{ifthen}    
\usepackage{lscape}

\DeclareMathOperator*{\Res}{Res}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}
\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}
\def\SPSB#1#2{\rlap{\textsuperscript{\textcolor{black}{#1}}}\SB{#2}}
\def\SP#1{\textsuperscript{\textcolor{black}{#1}}}
\def\SB#1{\textsubscript{\textcolor{black}{#1}}}                            

\lstset{
%language=python,
frame=single, 
breaklines=true,
columns=fullflexible
}

%\usepackage{amssymb}
%\usepackage{epstopdf}
%\title{Brief Article}
%\author {The Author}

\begin{document}
%\maketitle

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}

\bibliographystyle{IEEEtran}

\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}

\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\numberwithin{equation}{section}

\let\StandardTheFigure\thefigure
\let\StandardTheFigure\thefigure
\let\StandardTheTable\thetable

\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}
     
 \title{ Principal Component Analysis}
 
 \author{Nisha Akole, G V V 
Sharma$^{*}$% <-this % stops a space
\thanks{*The authors are with the Department
of Electrical Engineering, Indian Institute of Technology, Hyderabad
502285 India e-mail:  gadepall@iith.ac.in.}
}
\maketitle

\tableofcontents

\bigskip

\begin{abstract}
%\boldmath
This manual provides a brief description on how to implement Principal Component Analysis from scratch and use it for dimensionality reduction of data.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Objective}
Our objective is to implement PCA and reduce dimensionality of data which gives better understanding of data visually.

\section{Load Dataset}
The dataset used for PCA is available at the following link. Download all the data file in the folder where you want to write code for PCA.
\begin{lstlisting}
https://github.com/prabhatrai111/Commensal-Radar
\end{lstlisting}
\begin{lstlisting}
import numpy as np
import scipy.io as sio
from sklearn.utils.extmath import randomized_svd

mat_contents = sio.loadmat('data_all.mat')
X_data = mat_contents['data_all']
\end{lstlisting}

\section{About PCA}
Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.
\begin{lstlisting}
cov[PC1, PC2] = E[PC1PC2] - E[PC1]E[PC2]=0
\end{lstlisting}
Where, PC - principal components
There are four main operations in the PCA:\\
		1) Pre-processing\\
		2) Co-variance Matrix\\
		3) Eigen Vectors\\
		4) Feature Vector and Plot

\section{Pre-processing}
Feature scaling is an important task in PCA to get an optimized output. The functions  StandardScaler or MinMaxScaler will perform mean normalization by making mean $=0$ and variance $=1$.
StandardScaler Function : 
$$ z = (X-\mu)/\sigma$$
where, $\mu = $Mean of data X\\
$\sigma =$Standard Deviation of data X\\

\begin{lstlisting}[mathescape=true]
X = np.matrix(X_data)
$\mu = X.mean(0)$
$\sigma =X.std(0)$
X_std$ = (X-\mu)/\sigma$
\end{lstlisting}

%X.mean(0) and X.std(0) will give columnwise mean and standard deviation of our data matrix.

\section{Compute Covariance Matrix}
\[
   X\_cov=
  \begin{bmatrix}
   var_{PC1} & cov_{PC1,PC2} \\
   cov_{PC1,PC2} & var_{PC2} \\
  \end{bmatrix} = 
  \begin{bmatrix}
   \sigma\SPSB{2}{PC1} & \sigma_{PC1,PC2} \\
   \sigma_{PC1,PC2} & \sigma\SPSB{2}{PC2} \\
  \end{bmatrix} 
  \]

If variables are highly correlated, they contain redundant information.
\begin{center} $\mathbf{X\_cov}^\intercal$ = X\_cov \end{center} 
 
\begin{lstlisting}[mathescape=true]
X_cov = np.cov(X_std)
\end{lstlisting}

\section{Compute Eigenvectors}
Eigenvalues and eigevectors can be calculated by Singular Value Decomposition(SVD) of a covariance matrix. 
\begin{center} X\_cov = U $\Sigma$ $\mathbf{V}^\intercal$ \end{center}Eigenvalues are arranged in decreasing order in sigma matrix and its corresponding eigenvector is arranged in U.

\begin{center} $\mathbf{\Sigma}^\intercal$ = $\Sigma$ \end{center} 

\begin{lstlisting}[mathescape=true]
U, $\Sigma$, $\mathbf{V}^\intercal$ = randomized_svd(X_cov, n_components=2 ,n_iter= 5, random_state=none)
\end{lstlisting}

n\_components will consider first 2 largest eigenvalues. Hence, first 2 eigenvectors will be used for plotting the data. 
%these eigenvectors are known as principal components which carries maximum information.

\section{Principal Component and plot}
To visualize the data, only 2 or 3 principal components are needed.
To decide K principal components out of n to retain maximum information:

\begin{center}
$\frac{\sum\SPSB{k}{i=1}{First K Diagona Elements}}{\sum{All Diagonal Elements}} >= 0.99$
\end{center}

Link for PCA without function is given below:
\begin{lstlisting}[mathescape=true]
https://github.com/NishaAkole/AI-and-ML/blob/master/codes/pca/PCAwithoutFun.py
\end{lstlisting}
  
The above whole program can be combined except preprocessing and a single command(i.e using inbuilt function) can do all the  
\begin{lstlisting}[mathescape=true]
https://github.com/NishaAkole/AI-and-ML/blob/master/codes/pca/PCAFun.py
\end{lstlisting}

\section{Figures}
\begin{figure}[!h]
\begin{center}
\includegraphics[width=3in]{figs/PCA_2D.png}
\end{center}
\caption{PCA with Function 2D}
\label{fig: 2D Plot}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=3.8in]{figs/PCA_3D.png}
\end{center}
\caption{PCA with Function 3D}
\label{fig: 3D Plot}
\end{figure}

%\begin{figure}[!h]
%\begin{center}
%\includegraphics[width=3in]{figs/PCA_2D_woFun.png}
%\end{center}
%\caption{PCA without Function 2D}
%\label{fig: 2D Plot}
%\end{figure}

%\begin{figure}[!h]
%\begin{center}
%\includegraphics[width=3.8in]{figs/PCA_3D_woFun.png}
%\end{center}
%\caption{PCA without Function 3D}
%\label{fig: 3D Plot}
%\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=3.8in]{figs/Accuracy.png}
\end{center}
\caption{Information vs No of Dimensions }
\label{fig: }
\end{figure}  

From fig. 5, we are not able to reduce the dimensionality below 8000. Hence, PCA is not a good approach for dimensionality reduction for this data.

\end{document}

Â© 2019 GitHub, Inc.
Terms
Privacy
Security
Status
Help
Contact GitHub
Pricing
API
Training
Blog
About
